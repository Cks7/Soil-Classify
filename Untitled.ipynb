{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59c58ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "587e498d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 126 images belonging to 5 classes.\n",
      "Found 30 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "dataset_path = 'Soil types'  # Update this path\n",
    "\n",
    "# Image data generator for data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,  # Using 20% of data for validation\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Load training data\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Load validation data\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d85120d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Athar\\miniconda3\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(5, activation='softmax')  # 5 classes for 5 soil types\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fdd9aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b76116e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Athar\\miniconda3\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.1832 - loss: 1.9382 - val_accuracy: 0.2000 - val_loss: 1.6435\n",
      "Epoch 2/25\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 542ms/step - accuracy: 0.1250 - loss: 1.7754"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Athar\\miniconda3\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.1250 - loss: 1.7754 - val_accuracy: 0.2667 - val_loss: 1.4900\n",
      "Epoch 3/25\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.3595 - loss: 1.4386 - val_accuracy: 0.3667 - val_loss: 1.2934\n",
      "Epoch 4/25\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.3750 - loss: 1.4224 - val_accuracy: 0.4000 - val_loss: 1.2558\n",
      "Epoch 5/25\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.3911 - loss: 1.1482 - val_accuracy: 0.4000 - val_loss: 1.0424\n",
      "Epoch 6/25\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.4062 - loss: 1.0812 - val_accuracy: 0.4333 - val_loss: 0.9424\n",
      "Epoch 7/25\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.5026 - loss: 0.9660 - val_accuracy: 0.4667 - val_loss: 0.9067\n",
      "Epoch 8/25\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.8661 - val_accuracy: 0.5667 - val_loss: 0.8921\n",
      "Epoch 9/25\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 618ms/step - accuracy: 0.4989 - loss: 0.9645 - val_accuracy: 0.3667 - val_loss: 0.9639\n",
      "Epoch 10/25\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.9750 - val_accuracy: 0.7000 - val_loss: 0.8333\n",
      "Epoch 11/25\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.5924 - loss: 0.8346 - val_accuracy: 0.7000 - val_loss: 0.7444\n",
      "Epoch 12/25\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - accuracy: 0.7000 - loss: 0.7242 - val_accuracy: 0.6333 - val_loss: 0.7634\n",
      "Epoch 13/25\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 658ms/step - accuracy: 0.6589 - loss: 0.7817 - val_accuracy: 0.7000 - val_loss: 0.7741\n",
      "Epoch 14/25\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step - accuracy: 0.6667 - loss: 0.8239 - val_accuracy: 0.7333 - val_loss: 0.7538\n",
      "Epoch 15/25\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.7129 - loss: 0.7013 - val_accuracy: 0.7000 - val_loss: 0.7094\n",
      "Epoch 16/25\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.5938 - loss: 0.8162 - val_accuracy: 0.7667 - val_loss: 0.6317\n",
      "Epoch 17/25\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 624ms/step - accuracy: 0.6067 - loss: 0.7783 - val_accuracy: 0.7000 - val_loss: 0.7503\n",
      "Epoch 18/25\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.6875 - loss: 0.7688 - val_accuracy: 0.7333 - val_loss: 0.6309\n",
      "Epoch 19/25\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 626ms/step - accuracy: 0.7141 - loss: 0.6775 - val_accuracy: 0.6667 - val_loss: 0.6961\n",
      "Epoch 20/25\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.6250 - loss: 0.7162 - val_accuracy: 0.8000 - val_loss: 0.6056\n",
      "Epoch 21/25\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.8489 - loss: 0.5142 - val_accuracy: 0.7333 - val_loss: 0.5573\n",
      "Epoch 22/25\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - accuracy: 0.7188 - loss: 0.6468 - val_accuracy: 0.7333 - val_loss: 0.5625\n",
      "Epoch 23/25\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.8433 - loss: 0.4855 - val_accuracy: 0.8667 - val_loss: 0.4048\n",
      "Epoch 24/25\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - accuracy: 0.8750 - loss: 0.4316 - val_accuracy: 0.8000 - val_loss: 0.4321\n",
      "Epoch 25/25\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 609ms/step - accuracy: 0.7839 - loss: 0.5848 - val_accuracy: 0.8000 - val_loss: 0.5163\n"
     ]
    }
   ],
   "source": [
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=25,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b70e5b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 492ms/step - accuracy: 0.8333 - loss: 0.4094\n",
      "Validation Accuracy: 83.33%\n"
     ]
    }
   ],
   "source": [
    "# Load the best saved model\n",
    "best_model = tf.keras.models.load_model('best_model.keras')\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = best_model.evaluate(validation_generator)\n",
    "print(f'Validation Accuracy: {accuracy*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fa3a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model.save('soil_classification_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26a12ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step\n",
      "The predicted class for the image is: Yellow Soil\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess an image\n",
    "def load_and_preprocess_image(img_path, target_size=(150, 150)):\n",
    "    from tensorflow.keras.preprocessing import image\n",
    "    img = image.load_img(img_path, target_size=target_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array = img_array / 255.0  # Rescale to [0, 1]\n",
    "    return img_array\n",
    "\n",
    "# Predict the class of an image\n",
    "def predict_image_class(img_path, model, class_indices):\n",
    "    img_array = load_and_preprocess_image(img_path)\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
    "    class_labels = {v: k for k, v in class_indices.items()}\n",
    "    predicted_class_label = class_labels[predicted_class_index]\n",
    "    return predicted_class_label\n",
    "\n",
    "# Ensure that the best model is loaded\n",
    "best_model = tf.keras.models.load_model('best_model.keras')\n",
    "\n",
    "# Dictionary of class labels and their corresponding indices\n",
    "class_indices = train_generator.class_indices\n",
    "\n",
    "# Path to the image you want to classify\n",
    "img_path = 'Soil types/Yellow Soil/29.jpg'  # Update this path\n",
    "\n",
    "# Predict the class\n",
    "predicted_class = predict_image_class(img_path, best_model, class_indices)\n",
    "print(f'The predicted class for the image is: {predicted_class}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51a7530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
